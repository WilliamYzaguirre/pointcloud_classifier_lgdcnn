{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import os\n",
    "from path import Path\n",
    "import plotly.graph_objects as go\n",
    "import plotly.express as px\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import CosineAnnealingLR\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms, utils\n",
    "import sklearn.metrics as metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "visualization tools. doesn't work with cude tensors... duh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_rotate(data):\n",
    "    x_eye, y_eye, z_eye = 1.25, 1.25, 0.8\n",
    "    frames=[]\n",
    "\n",
    "    def rotate_z(x, y, z, theta):\n",
    "        w = x+1j*y\n",
    "        return np.real(np.exp(1j*theta)*w), np.imag(np.exp(1j*theta)*w), z\n",
    "\n",
    "    for t in np.arange(0, 10.26, 0.1):\n",
    "        xe, ye, ze = rotate_z(x_eye, y_eye, z_eye, -t)\n",
    "        frames.append(dict(layout=dict(scene=dict(camera=dict(eye=dict(x=xe, y=ye, z=ze))))))\n",
    "    fig = go.Figure(data=data,\n",
    "                    layout=go.Layout(\n",
    "                        updatemenus=[dict(type='buttons',\n",
    "                                    showactive=False,\n",
    "                                    y=1,\n",
    "                                    x=0.8,\n",
    "                                    xanchor='left',\n",
    "                                    yanchor='bottom',\n",
    "                                    pad=dict(t=45, r=10),\n",
    "                                    buttons=[dict(label='Play',\n",
    "                                                    method='animate',\n",
    "                                                    args=[None, dict(frame=dict(duration=50, redraw=True),\n",
    "                                                                    transition=dict(duration=0),\n",
    "                                                                    fromcurrent=True,\n",
    "                                                                    mode='immediate'\n",
    "                                                                    )]\n",
    "                                                    )\n",
    "                                            ]\n",
    "                                    )\n",
    "                                ]\n",
    "                    ),\n",
    "                    frames=frames\n",
    "            )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "def pcshow(xs,ys,zs):\n",
    "    data=[go.Scatter3d(x=xs, y=ys, z=zs,\n",
    "                                   mode='markers')]\n",
    "    fig = visualize_rotate(data)\n",
    "    fig.update_traces(marker=dict(size=2,\n",
    "                      line=dict(width=2,\n",
    "                      color='DarkSlateGrey')),\n",
    "                      selector=dict(mode='markers'))\n",
    "    fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "reads .off file (cad model). returns the list of vertices, and the faces that they make"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_cad(file):\n",
    "    if 'OFF' != file.readline().strip():\n",
    "        raise('Not a valid .OFF file')\n",
    "    n_verts, n_faces, __ = tuple([int(s) for s in file.readline().strip().split(' ')])\n",
    "    verts = [[float(s) for s in file.readline().strip().split(' ')] for i_vert in range(n_verts)]\n",
    "    faces = [[int(s) for s in file.readline().strip().split(' ')][1:] for i_face in range(n_faces)]\n",
    "    return verts, faces"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "randomly samples points on the faces of object. output_size is the number of points you want in your pointcloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointSampler(object):\n",
    "    def __init__(self, output_size):\n",
    "        assert isinstance(output_size, int)\n",
    "        self.output_size = output_size\n",
    "    \n",
    "    def triangle_area(self, pt1, pt2, pt3):\n",
    "        side_a = np.linalg.norm(pt1 - pt2)\n",
    "        side_b = np.linalg.norm(pt2 - pt3)\n",
    "        side_c = np.linalg.norm(pt3 - pt1)\n",
    "        s = 0.5 * ( side_a + side_b + side_c)\n",
    "        return max(s * (s - side_a) * (s - side_b) * (s - side_c), 0)**0.5\n",
    "\n",
    "    def sample_point(self, pt1, pt2, pt3):\n",
    "        s, t = sorted([random.random(), random.random()])\n",
    "        f = lambda i: s * pt1[i] + (t-s)*pt2[i] + (1-t)*pt3[i]\n",
    "        return (f(0), f(1), f(2))\n",
    "        \n",
    "    \n",
    "    def __call__(self, mesh):\n",
    "        verts, faces = mesh\n",
    "        verts = np.array(verts)\n",
    "        areas = np.zeros((len(faces)))\n",
    "\n",
    "        for i in range(len(areas)):\n",
    "            areas[i] = (self.triangle_area(verts[faces[i][0]],\n",
    "                                           verts[faces[i][1]],\n",
    "                                           verts[faces[i][2]]))\n",
    "            \n",
    "        sampled_faces = (random.choices(faces, \n",
    "                                      weights=areas,\n",
    "                                      cum_weights=None,\n",
    "                                      k=self.output_size))\n",
    "        \n",
    "        sampled_points = np.zeros((self.output_size, 3), dtype=np.float32)\n",
    "\n",
    "        for i in range(len(sampled_faces)):\n",
    "            sampled_points[i] = (self.sample_point(verts[sampled_faces[i][0]],\n",
    "                                                   verts[sampled_faces[i][1]],\n",
    "                                                   verts[sampled_faces[i][2]]))\n",
    "        \n",
    "        return sampled_points\n",
    "    \n",
    "class Normalize(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        \n",
    "        norm_pointcloud = pointcloud - np.mean(pointcloud, axis=0) \n",
    "        norm_pointcloud /= np.max(np.linalg.norm(norm_pointcloud, axis=1))\n",
    "\n",
    "        return  norm_pointcloud\n",
    "    \n",
    "class RandRotation_z(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "\n",
    "        theta = random.random() * 2. * math.pi\n",
    "        rot_matrix = np.array([[ math.cos(theta), -math.sin(theta),    0],\n",
    "                               [ math.sin(theta),  math.cos(theta),    0],\n",
    "                               [0,                             0,      1]])\n",
    "        \n",
    "        rot_pointcloud = rot_matrix.dot(pointcloud.T).T\n",
    "        return  rot_pointcloud\n",
    "    \n",
    "class RandomNoise(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "\n",
    "        noise = np.random.normal(0, 0.02, (pointcloud.shape))\n",
    "    \n",
    "        noisy_pointcloud = pointcloud + noise\n",
    "        return  noisy_pointcloud\n",
    "    \n",
    "class ToTensor(object):\n",
    "    def __call__(self, pointcloud):\n",
    "        assert len(pointcloud.shape)==2\n",
    "        return torch.from_numpy(pointcloud).type(torch.float16)\n",
    "    \n",
    "def default_transforms():\n",
    "    return transforms.Compose([\n",
    "                                PointSampler(512),\n",
    "                                Normalize(),\n",
    "                                ToTensor()\n",
    "                              ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PointCloudData(Dataset):\n",
    "    def __init__(self, root_dir, valid=False, folder=\"train\", transform=default_transforms()):\n",
    "        self.root_dir = root_dir\n",
    "        folders = [dir for dir in sorted(os.listdir(root_dir)) if os.path.isdir(root_dir/dir)]\n",
    "        self.classes = {folder: i for i, folder in enumerate(folders)}\n",
    "        self.transforms = transform if not valid else default_transforms()\n",
    "        self.valid = valid\n",
    "        self.files = []\n",
    "        for category in self.classes.keys():\n",
    "            new_dir = root_dir/Path(category)/folder\n",
    "            for file in os.listdir(new_dir):\n",
    "                if file.endswith('.off'):\n",
    "                    sample = {}\n",
    "                    sample['pcd_path'] = new_dir/file\n",
    "                    sample['category'] = category\n",
    "                    self.files.append(sample)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __preproc__(self, file):\n",
    "        verts, faces = read_cad(file)\n",
    "        if self.transforms:\n",
    "            pointcloud = self.transforms((verts, faces))\n",
    "        return pointcloud\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        pcd_path = self.files[idx]['pcd_path']\n",
    "        category = self.files[idx]['category']\n",
    "        with open(pcd_path, 'r') as f:\n",
    "            pointcloud = self.__preproc__(f)\n",
    "        return {'pointcloud': pointcloud, \n",
    "                'category': self.classes[category]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The transformations to be applied to the training data. makes the pointcloud, normalizes it, randomly rotates on z axis, adds random gaussian noise, then converts it to a cuda tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = transforms.Compose([\n",
    "                    PointSampler(512),\n",
    "                    Normalize(),\n",
    "                    RandRotation_z(),\n",
    "                    RandomNoise(),\n",
    "                    ToTensor()\n",
    "                    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "training and validation datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = Path(\"ModelNet10/ModelNet10\")\n",
    "train_ds = PointCloudData(path, transform=train_transforms)\n",
    "valid_ds = PointCloudData(path, valid=True, folder='test', transform=train_transforms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'bathtub', 1: 'bed', 2: 'chair', 3: 'desk', 4: 'dresser', 5: 'monitor', 6: 'night_stand', 7: 'sofa', 8: 'table', 9: 'toilet'}\n"
     ]
    }
   ],
   "source": [
    "inv_classes = {i: cat for cat, i in train_ds.classes.items()};\n",
    "print(inv_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets look at the stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size:  3991\n",
      "Valid dataset size:  908\n",
      "Number of classes:  10\n",
      "Sample pointcloud shape:  torch.Size([512, 3])\n",
      "Class:  chair\n",
      "tensor([[-0.3884, -0.3628,  0.5166],\n",
      "        [-0.5010, -0.0956, -0.0082],\n",
      "        [ 0.4780, -0.3306,  0.3838],\n",
      "        ...,\n",
      "        [ 0.0907,  0.4031, -0.0163],\n",
      "        [ 0.2844,  0.0309, -0.4441],\n",
      "        [-0.2378, -0.2925,  0.3638]], dtype=torch.float16)\n"
     ]
    }
   ],
   "source": [
    "print('Train dataset size: ', len(train_ds))\n",
    "print('Valid dataset size: ', len(valid_ds))\n",
    "print('Number of classes: ', len(train_ds.classes))\n",
    "print('Sample pointcloud shape: ', train_ds[1000]['pointcloud'].size())\n",
    "print('Class: ', inv_classes[train_ds[1000]['category']])\n",
    "print(train_ds[1000]['pointcloud'])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "make sure it's cuda and everything looks correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.float16\n"
     ]
    }
   ],
   "source": [
    "print(train_ds[1000]['pointcloud'].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "let's start the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def knn(x, k):\n",
    "    inner = -2*torch.matmul(x.transpose(2,1), x)\n",
    "    xx = torch.sum(x**2, dim=1, keepdim=True)\n",
    "    pairwise_distance = -xx - inner - xx.transpose(2,1)\n",
    "    \n",
    "    idx = pairwise_distance.topk(k=k, dim=-1)[1]\n",
    "    return idx\n",
    "\n",
    "def get_graph_feature(x, k, idx=None):\n",
    "    batch_size = x.size(0)\n",
    "    num_points = x.size(2)\n",
    "    x = x.view(batch_size, -1, num_points)\n",
    "    if idx is None:\n",
    "        idx = knn(x, k=k)   # (batch_size, num_points, k)\n",
    "    device = torch.device('cuda')\n",
    "\n",
    "    idx_base = torch.arange(0, batch_size, device=device).view(-1, 1, 1)*num_points\n",
    "    \n",
    "    idx = idx + idx_base\n",
    "\n",
    "    idx = idx.view(-1)\n",
    "    \n",
    "    _, num_dims, _ = x.size()\n",
    "\n",
    "    x = x.transpose(2, 1).contiguous()   # (batch_size, num_points, num_dims)  -> (batch_size*num_points, num_dims) \n",
    "\n",
    "    feature = x.view(batch_size*num_points, -1)[idx, :] # batch_size * num_points * k + range(0, batch_size*num_points)\n",
    "    feature = feature.view(batch_size, num_points, k, num_dims) \n",
    "    x = x.view(batch_size, num_points, 1, num_dims).repeat(1, 1, k, 1)\n",
    "    \n",
    "    feature = torch.cat((feature-x, x), dim=3).permute(0, 3, 1, 2).contiguous()\n",
    "      \n",
    "    return feature\n",
    "\n",
    "class DGCNN(nn.Module):\n",
    "    def __init__(self, dropout=0.5, k=20, emb_dims=1024, output_channels=40):\n",
    "        super(DGCNN, self).__init__()\n",
    "        self.dropout = dropout\n",
    "        self.k = k\n",
    "        self.emb_dims = emb_dims\n",
    "        \n",
    "        self.bn1 = nn.BatchNorm2d(64)\n",
    "        self.bn2 = nn.BatchNorm2d(64)\n",
    "        self.bn3 = nn.BatchNorm2d(128)\n",
    "        self.bn4 = nn.BatchNorm2d(256)\n",
    "        self.bn5 = nn.BatchNorm1d(self.emb_dims)\n",
    "\n",
    "        self.conv1 = nn.Sequential(nn.Conv2d(6, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn1,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv2 = nn.Sequential(nn.Conv2d(64*2, 64, kernel_size=1, bias=False),\n",
    "                                   self.bn2,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv3 = nn.Sequential(nn.Conv2d(64*2, 128, kernel_size=1, bias=False),\n",
    "                                   self.bn3,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv4 = nn.Sequential(nn.Conv2d(128*2, 256, kernel_size=1, bias=False),\n",
    "                                   self.bn4,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.conv5 = nn.Sequential(nn.Conv1d(512, self.emb_dims, kernel_size=1, bias=False),\n",
    "                                   self.bn5,\n",
    "                                   nn.LeakyReLU(negative_slope=0.2))\n",
    "        self.linear1 = nn.Linear(self.emb_dims*2, 512, bias=False)\n",
    "        self.bn6 = nn.BatchNorm1d(512)\n",
    "        self.dp1 = nn.Dropout(p=self.dropout)\n",
    "        self.linear2 = nn.Linear(512, 256)\n",
    "        self.bn7 = nn.BatchNorm1d(256)\n",
    "        self.dp2 = nn.Dropout(p=self.dropout)\n",
    "        self.linear3 = nn.Linear(256, output_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x['pointcloud']\n",
    "        batch_size = x.size(0)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = get_graph_feature(x, k=self.k)\n",
    "        x = self.conv1(x)\n",
    "        x1 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x1, k=self.k)\n",
    "        x = self.conv2(x)\n",
    "        x2 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x2, k=self.k)\n",
    "        x = self.conv3(x)\n",
    "        x3 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = get_graph_feature(x3, k=self.k)\n",
    "        x = self.conv4(x)\n",
    "        x4 = x.max(dim=-1, keepdim=False)[0]\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), dim=1)\n",
    "\n",
    "        x = self.conv5(x)\n",
    "        x1 = F.adaptive_max_pool1d(x, 1).view(batch_size, -1)\n",
    "        x2 = F.adaptive_avg_pool1d(x, 1).view(batch_size, -1)\n",
    "        x = torch.cat((x1, x2), 1)\n",
    "\n",
    "        x = F.leaky_relu(self.bn6(self.linear1(x)), negative_slope=0.2)\n",
    "        x = self.dp1(x)\n",
    "        x = F.leaky_relu(self.bn7(self.linear2(x)), negative_slope=0.2)\n",
    "        x = self.dp2(x)\n",
    "        x = self.linear3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_loss(pred, gold, smoothing=True):\n",
    "    gold = gold.contiguous().view(-1)\n",
    "\n",
    "    if smoothing:\n",
    "        eps = 0.2\n",
    "        n_class = pred.size(1)\n",
    "\n",
    "        one_hot = torch.zeros_like(pred).scatter(1, gold.view(-1, 1).type(torch.int64), 1)\n",
    "        one_hot = one_hot * (1 - eps) + (1 - one_hot) * eps / (n_class - 1)\n",
    "        log_prb = F.log_softmax(pred, dim=1)\n",
    "\n",
    "        loss = -(one_hot * log_prb).sum(dim=1).mean()\n",
    "    else:\n",
    "        loss = F.cross_entropy(pred, gold, reduction='mean')\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 250\n",
    "learning_rate = 0.001\n",
    "torch.set_default_dtype(torch.float16)\n",
    "exp_name = \"Test1\"\n",
    "\n",
    "def train():\n",
    "    train_loader = DataLoader(dataset=train_ds, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(dataset=valid_ds, batch_size=16, shuffle=True)\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    \n",
    "    model = DGCNN().to(device)\n",
    "    model = nn.DataParallel(model)\n",
    "    \n",
    "    opt = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "    scheduler = CosineAnnealingLR(opt, epochs, eta_min=learning_rate)\n",
    "    \n",
    "    criterion = cal_loss\n",
    "    \n",
    "    best_test_acc = 0\n",
    "    for epoch in range(epochs):\n",
    "        ######### TRAIN ############\n",
    "        train_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.train()\n",
    "        train_pred = []\n",
    "        train_true = []\n",
    "        for i, data in enumerate(train_loader, 0):\n",
    "            #if (i % 20 == 0):\n",
    "            print(i)\n",
    "            inputs, labels = data['pointcloud'].to(device), data['category'].type(torch.uint8).to(device).squeeze()\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            batch_size = inputs.size()[0]\n",
    "            opt.zero_grad()\n",
    "            logits = model(data)\n",
    "            loss = criterion(logits, labels)\n",
    "            loss.backward()\n",
    "            opt.step()\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            train_loss += float(loss.item()) * batch_size\n",
    "            train_true.append(labels.cpu().numpy())\n",
    "            train_pred.append(preds.detach().cpu().numpy())\n",
    "        train_true = np.concatenate(train_true)\n",
    "        train_pred = np.concatenate(train_pred)\n",
    "        print('Train %d, loss: %.6f, train acc: %.6f, train avg acc: %.6f' % (epoch, train_loss*1.0/count, metrics.accuracy_score(train_true, train_pred), metrics.balanced_accuracy_score(train_true, train_pred)))\n",
    "        \n",
    "        ########### VALID ####################\n",
    "        test_loss = 0.0\n",
    "        count = 0.0\n",
    "        model.eval()\n",
    "        test_pred = []\n",
    "        test_true = []\n",
    "        for i, data in enumerate(test_loader, 0):\n",
    "            inputs, labels = data['pointcloud'].to(device), data['category'].type(torch.uint8).to(device).squeeze()\n",
    "            inputs = inputs.permute(0, 2, 1)\n",
    "            batch_size = inputs.size()[0]\n",
    "            logits = model(inputs)\n",
    "            loss = criterion(logits, labels)\n",
    "            preds = logits.max(dim=1)[1]\n",
    "            count += batch_size\n",
    "            test_loss += float(loss.item()) * batch_size\n",
    "            test_true.append(labels.cpu().numpy())\n",
    "            test_pred.append(preds.detach().cpu().numpy())\n",
    "        test_true = np.concatenate(test_true)\n",
    "        test_pred = np.concatenate(test_pred)\n",
    "        test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "        avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "        print('Test %d, loss: %.6f, test acc: %.6f, test avg acc: %.6f' % (epoch, test_loss*1.0/count, test_acc, avg_per_class_acc))\n",
    "        \n",
    "        if test_acc >= best_test_acc:\n",
    "            best_test_acc = test_acc\n",
    "            #torch.save(model.state_dict(), 'checkpoints/%s/models/model.t7' % exp_name)\n",
    "        scheduler.step()\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    test_loader = DataLoader(dataset=valid_ds, batch_size=16, num_workers=4, shuffle=True)\n",
    "\n",
    "\n",
    "    #device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    #model = DGCNN().to(device)\n",
    "    #model = nn.DataParallel(model)\n",
    "    #model.load_state_dict(torch.load(args.model_path))\n",
    "    model = model.eval()\n",
    "    test_acc = 0.0\n",
    "    count = 0.0\n",
    "    test_true = []\n",
    "    test_pred = []\n",
    "    for i, data in enumerate(test_loader, 0):\n",
    "        inputs, labels = data['pointcloud'].to(device), data['category'].type(torch.uint8).to(device).squeeze()\n",
    "        inputs = inputs.permute(0, 2, 1)\n",
    "        batch_size = inputs.size()[0]\n",
    "        logits = model(inputs)\n",
    "        preds = logits.max(dim=1)[1]\n",
    "        test_true.append(labels.cpu().numpy())\n",
    "        test_pred.append(preds.detach().cpu().numpy())\n",
    "    test_true = np.concatenate(test_true)\n",
    "    test_pred = np.concatenate(test_pred)\n",
    "    test_acc = metrics.accuracy_score(test_true, test_pred)\n",
    "    avg_per_class_acc = metrics.balanced_accuracy_score(test_true, test_pred)\n",
    "    print('Test :: test acc: %.6f, test avg acc: %.6f'%(test_acc, avg_per_class_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()\n",
    "print(\"ji\")\n",
    "train()\n",
    "test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import gc\n",
    "#del model\n",
    "#gc.collect()\n",
    "#torch.cuda.empty_cache()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
